{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Named Entity Recognition and Relation Extraction using DSPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default definition for model usage and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from collections import Counter\n",
    "from devtools import pprint\n",
    "from dotenv import load_dotenv\n",
    "from dspy.functional import TypedPredictor\n",
    "from dspy.teleprompt import LabeledFewShot\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Union\n",
    "import ast\n",
    "import dspy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "assert load_dotenv(\"/work3/s174159/LLM_Thesis/prompt/.env\") == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API = os.getenv('OPENAI')\n",
    "HF = os.getenv(\"HF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = dspy.OpenAI(model='gpt-4o-mini', max_tokens=2048, api_key=API)\n",
    "gpt4 = dspy.OpenAI(model='gpt-4', max_tokens=1024, api_key= API)\n",
    "gpt3 = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=2048, api_key=API)\n",
    "\n",
    "\n",
    "# Set default model\n",
    "dspy.settings.configure(lm=gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample dataset and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FROM_FILES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_FROM_FILES:\n",
    "    test_input = pd.read_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/test_sampled.json\")\n",
    "    dev_input = pd.read_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/dev_sampled.json\")\n",
    "    sampled_df = pd.read_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/train_sampled.json\")\n",
    "\n",
    "df1 = pd.read_json('/work3/s174159/LLM_Thesis/data/Final/Combined/train_annotated.json')\n",
    "#df2 = pd.read_json(\"/work3/s174159/LLM_Thesis/data/No_Constraints/Data_Finalized/Combined/train_distant.json\")\n",
    "dataframes = [df1]\n",
    "df = pd.concat(dataframes)\n",
    "df['org_dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equal_per_class(df, sample_size, column_name):\n",
    "    unique_classes = df[column_name].unique()\n",
    "    n_samples_per_class = sample_size // len(unique_classes)  # Equal samples per class\n",
    "\n",
    "    sampled_df = pd.DataFrame()\n",
    "\n",
    "    # Sample equal number of samples from each class\n",
    "    for category in unique_classes:\n",
    "        sampled_df = pd.concat([sampled_df, df[df[column_name] == category].sample(n=n_samples_per_class, random_state=42)])\n",
    "\n",
    "    return sampled_df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FROM_FILES:\n",
    "    sampled_df = sample_equal_per_class(df, SAMPLE_SIZE, 'org_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distribution = df['org_dataset'].value_counts(normalize=True) * 100\n",
    "sampled_distribution = sampled_df['org_dataset'].value_counts(normalize=True) * 100\n",
    "\n",
    "\n",
    "distribution_df = pd.DataFrame({'Original': original_distribution, 'Sampled': sampled_distribution}).sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "distribution_df.plot(kind='bar', width=0.8)\n",
    "plt.title('Distribution of classes in the original and sampled dataset')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    # Prepare the DSPY dataset\n",
    "    dataset = [None] * len(df.values)\n",
    "    for i,(org_dataset, title, domains, sents, vertexSet, labels, original_file_path) in tqdm(enumerate(df.values), total=len(df), desc=\"Creating dataset - Processing row\"):\n",
    "        # Convert JSON strings to Python objects\n",
    "        sents = (sents)\n",
    "        vertexSet = (vertexSet)\n",
    "        labels = (labels)\n",
    "        \n",
    "        entity_types = list(set([entity['type'] for entity_list in vertexSet for entity in entity_list]))\n",
    "        relation_types = list(set([relation['r'] for relation in labels]))\n",
    "        \n",
    "        # Create a DSPY example and append to the dataset\n",
    "        example = dspy.Example(\n",
    "            sentences=str(sents), \n",
    "            entities=vertexSet, \n",
    "            relations=labels\n",
    "        ).with_inputs('sentences', 'entity_types', 'relation_types')\n",
    "\n",
    "        # Attach entity_types and relation_types to the example\n",
    "        example['entity_types'] = str(entity_types)\n",
    "        example['relation_types'] = str(relation_types)\n",
    "        \n",
    "        dataset[i] = example\n",
    "\n",
    "    # Now the dataset is ready for use with DSPY\n",
    "    print(f\"Total examples created: {len(dataset)}\")\n",
    "\n",
    "    # Check one example to see the structure\n",
    "    print(dataset[0])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(sampled_df)\n",
    "dev_data = prepare_dataset(dev_input)\n",
    "test_data = prepare_dataset(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FROM_FILES:\n",
    "    dev_input = pd.read_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/dev.json\")\n",
    "    dev_input = sample_equal_per_class(dev_input, SAMPLE_SIZE, 'org_dataset')\n",
    "    dev_data = prepare_dataset(dev_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FROM_FILES:\n",
    "    test_input = pd.read_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/test.json\")\n",
    "    test_input = sample_equal_per_class(test_input, SAMPLE_SIZE, 'org_dataset')\n",
    "    test_input.to_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/test_sampled.json\")\n",
    "    dev_input.to_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/dev_sampled.json\")\n",
    "    sampled_df.to_json(\"/work3/s174159/LLM_Thesis/data/Final/Combined/train_sampled.json\")\n",
    "    test_data = prepare_dataset(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DSPY Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity_extractor(dspy.Signature):\n",
    "    \"\"\"Extract entities from a provided text based on specified entity types.\n",
    "    The output is a list of tuples in the format (entity, type, location, evidence), where:\n",
    "    - 'entity' is the extracted term,\n",
    "    - 'type' is its classification (e.g., Person, Location),\n",
    "    - 'location' is the index range within the text where the entity is found, represented as a list [Start, End],\n",
    "    - 'evidence' is a list of indices indicating the specific sublists (within the list of lists) that contain the entity.\"\"\"\n",
    "    \n",
    "    sentences = dspy.InputField(description=\"A list of lists, where each inner list contains text segments to extract entities from.\", type=str, required=True)\n",
    "    entity_types = dspy.InputField(description=\"The types of entities to extract (e.g., Person, Location, Organization).\", type=str, required=True)\n",
    "    answer = dspy.OutputField(description=\"A string representing the list of extracted entities in the format (entity, type, location, evidence).\", type=str)\n",
    "\n",
    "\n",
    "\n",
    "class TupleToJson(dspy.Signature):\n",
    "    \"\"\"Convert extracted entity tuples into JSON format.\n",
    "    The output is a list of dictionaries, each containing:\n",
    "    - 'entity': the extracted term,\n",
    "    - 'type': its classification (e.g., Person, Location),\n",
    "    - 'location': the index range within the text where the entity is found,\n",
    "    - 'evidence': a list of indices indicating the sublists containing the entity.\n",
    "    The output should be formatted as a JSON list.\"\"\"\n",
    "    \n",
    "    text = dspy.InputField(description=\"Input text containing extracted entities in tuple format, i.e., (entity, type, location, evidence).\", type=str, required=True)\n",
    "    answer = dspy.OutputField(description=\"JSON-formatted output as a list of dictionaries. Remember to include [ ] to mark the list\", type=str)\n",
    "\n",
    "\n",
    "    \n",
    "class RelationExtractor(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given the text in form of Python lists of sentences, a list of dictionaries with entities and a list of relation types find the relations between the entities.\n",
    "    The output should be  a list of tuples in the format (entity1, entity2, relation), where:\n",
    "    - 'entity1' and 'entity2' are the names or terms representing the two entities involved,\n",
    "    - 'relation' is the specific relationship type between them, as defined in the provided relation types. Use the exact types as defined in the input field, relation_types. \n",
    "    This format captures the relationships directly as they appear in the text.\"\"\"\n",
    "    \n",
    "    entities = dspy.InputField(description=\"A list of entities in dictionary format [{'entity': name, 'type': entity_type, 'location': index}].\", type=str, required=True)\n",
    "    sentences = dspy.InputField(description=\"The original text containing the entities to identify relationships between them.\", type=str, required=True)\n",
    "    relation_types = dspy.InputField(description=\"Optional: Specific types of relationships to extract between the entities (e.g., Parent-Child, Colleague).\", type=str, required=False)\n",
    "    answer = dspy.OutputField(description=\"A string representing the list of identified relations in the format (entity1, entity2, relation).\", type=str)\n",
    "\n",
    "class TupleToJsonRelation(dspy.Signature):\n",
    "    \"\"\"Convert text into a structured JSON format representing relationships between entities.\n",
    "    The output should be a list of dictionaries, each containing:\n",
    "    - 'entity1': the first entity in the relationship,\n",
    "    - 'entity2': the second entity in the relationship,\n",
    "    - 'relation': the type of relationship between the entities.\n",
    "    The output should be formatted as a JSON list.\"\"\"\n",
    "    \n",
    "    text = dspy.InputField(description=\"Input text containing entity relationships described in natural language.\", type=str, required=True)\n",
    "    answer = dspy.OutputField(description=\"JSON-formatted output as a list of dictionaries, where each dictionary contains 'entity1', 'entity2', and 'relation type'. Do not include ```json tags. But only the json list itself\", type=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTRelationExtraction(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.entity_extraction = dspy.ChainOfThought(Entity_extractor)\n",
    "        self.tuple_to_json = dspy.ChainOfThought(TupleToJson)\n",
    "        self.relation_extraction = dspy.ChainOfThought(RelationExtractor)\n",
    "        self.tuple_to_json_relation = dspy.ChainOfThought(TupleToJsonRelation)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, sentences, entity_types, relation_types):\n",
    "        entities = self.entity_extraction(sentences=sentences, entity_types=entity_types)\n",
    "        entities = entities.answer.split(\"---\")[0]\n",
    "        entities = self.tuple_to_json(text=entities)\n",
    "        entities = str(entities.answer)\n",
    "        relations = self.relation_extraction(entities=entities, sentences=sentences, relation_types = relation_types)\n",
    "        # print(relations)\n",
    "        relations = relations.answer.split(\"---\")[0]\n",
    "        # print(relations)\n",
    "        relations = self.tuple_to_json_relation(text=relations)\n",
    "        relations = str(relations.answer)\n",
    "        # print(relations)\n",
    "        return str(relations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_extractor = CoTRelationExtraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_rels = []\n",
    "#results = []\n",
    "\n",
    "\n",
    "def _parse_input(input_data):\n",
    "    if isinstance(input_data, str):\n",
    "        try:\n",
    "            # Try to parse as JSON\n",
    "            return json.loads(input_data)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # Fallback to ast.literal_eval\n",
    "                return ast.literal_eval(input_data)\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Return None if both parsing attempts fail\n",
    "                return None\n",
    "    elif isinstance(input_data, dict):\n",
    "        # Wrap dict in a list\n",
    "        return [input_data]\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def combined_relation_f1_score(expected, predicted, trace=None):\n",
    "    \"\"\"\n",
    "    The following is only used to establish a baseline and thus re-trace the steps!\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse expected and predicted inputs\n",
    "    expected = _parse_input(expected)\n",
    "    predicted = _parse_input(predicted)\n",
    "\n",
    "    if expected is None:\n",
    "        return (\n",
    "            {\n",
    "                \"micro_f1\": 0,\n",
    "                \"micro_precision\": 0,\n",
    "                \"micro_recall\": 0,\n",
    "                \"macro_f1\": 0,\n",
    "                \"macro_precision\": 0,\n",
    "                \"macro_recall\": 0,\n",
    "                \"accuracy\": 0,\n",
    "                \"class_metrics\": {},\n",
    "            },\n",
    "            {\n",
    "                \"labels\": [],\n",
    "                \"predictions\": [],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Handle case where predicted or expected is None or empty\n",
    "    if len(predicted) == 0 and len(expected) == 0:\n",
    "        # Return zero-valued metrics\n",
    "        return (\n",
    "            {\n",
    "                \"micro_f1\": 0,\n",
    "                \"micro_precision\": 0,\n",
    "                \"micro_recall\": 0,\n",
    "                \"macro_f1\": 0,\n",
    "                \"macro_precision\": 0,\n",
    "                \"macro_recall\": 0,\n",
    "                \"accuracy\": 0,\n",
    "                \"class_metrics\": {},\n",
    "            },\n",
    "            {\"labels\": [], \"predictions\": []},\n",
    "        )\n",
    "    elif len(expected) == 0:\n",
    "        return (\n",
    "            {\n",
    "                \"micro_f1\": 0,\n",
    "                \"micro_precision\": 0,\n",
    "                \"micro_recall\": 0,\n",
    "                \"macro_f1\": 0,\n",
    "                \"macro_precision\": 0,\n",
    "                \"macro_recall\": 0,\n",
    "                \"accuracy\": 0,\n",
    "                \"class_metrics\": {},\n",
    "            },\n",
    "            {\n",
    "                \"labels\": [],\n",
    "                \"predictions\": [\n",
    "                    {\n",
    "                        \"head\": rel[\"entity1\"],\n",
    "                        \"tail\": rel[\"entity2\"],\n",
    "                        \"type\": rel[\"relation\"],\n",
    "                        \"correct\": False,\n",
    "                    }\n",
    "                    for rel in predicted\n",
    "                ],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    expected_relations = set()\n",
    "\n",
    "    # Extract expected relations from entities and relations\n",
    "    entities = expected.get(\"entities\", [])\n",
    "    relations = expected.get(\"relations\", [])\n",
    "\n",
    "    flat_entities = [entity for sublist in entities for entity in sublist]\n",
    "    for rel in relations:\n",
    "        entity1_name = flat_entities[rel[\"h\"]][\"name\"]\n",
    "        entity2_name = flat_entities[rel[\"t\"]][\"name\"]\n",
    "        relation_type = rel[\"r\"]\n",
    "        expected_relations.add((entity1_name, entity2_name, relation_type))\n",
    "\n",
    "    if predicted is None or len(predicted) == 0:\n",
    "        return (\n",
    "            {\n",
    "                \"micro_f1\": 0,\n",
    "                \"micro_precision\": 0,\n",
    "                \"micro_recall\": 0,\n",
    "                \"macro_f1\": 0,\n",
    "                \"macro_precision\": 0,\n",
    "                \"macro_recall\": 0,\n",
    "                \"accuracy\": 0,\n",
    "                \"class_metrics\": {},\n",
    "            },\n",
    "            {\n",
    "                \"labels\": [\n",
    "                    {\"head\": head, \"tail\": tail, \"type\": _type, \"correct\": False}\n",
    "                    for (head, tail, _type) in expected_relations\n",
    "                ],\n",
    "                \"predictions\": [],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Extract predicted relations\n",
    "    predicted_relations = {\n",
    "        (rel[\"entity1\"], rel[\"entity2\"], rel[\"relation\"]) for rel in predicted\n",
    "    }\n",
    "\n",
    "    # Create all possible labels for binary classification (1 if relation is in expected, 0 otherwise)\n",
    "    all_relations = list(expected_relations | predicted_relations)\n",
    "\n",
    "    y_true = [1 if rel in expected_relations else 0 for rel in all_relations]\n",
    "    y_pred = [1 if rel in predicted_relations else 0 for rel in all_relations]\n",
    "\n",
    "    # Check if y_true and y_pred are non-empty before calculating metrics\n",
    "    if len(y_true) == 0 or len(y_pred) == 0:\n",
    "        return (\n",
    "            {\n",
    "                \"micro_f1\": 0,\n",
    "                \"micro_precision\": 0,\n",
    "                \"micro_recall\": 0,\n",
    "                \"macro_f1\": 0,\n",
    "                \"macro_precision\": 0,\n",
    "                \"macro_recall\": 0,\n",
    "                \"accuracy\": 0,\n",
    "                \"class_metrics\": {},\n",
    "            },\n",
    "            {\n",
    "                \"labels\": [\n",
    "                    {\n",
    "                        \"head\": head,\n",
    "                        \"tail\": tail,\n",
    "                        \"type\": _type,\n",
    "                        \"correct\": (head, tail, _type) in predicted_relations,\n",
    "                    }\n",
    "                    for (head, tail, _type) in expected_relations\n",
    "                ],\n",
    "                \"predictions\": [\n",
    "                    {\n",
    "                        \"head\": head,\n",
    "                        \"tail\": tail,\n",
    "                        \"type\": _type,\n",
    "                        \"correct\": (head, tail, _type) in expected_relations,\n",
    "                    }\n",
    "                    for (head, tail, _type) in predicted_relations\n",
    "                ],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # Micro F1, precision, recall\n",
    "    metrics[\"micro_f1\"] = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    metrics[\"micro_precision\"] = precision_score(\n",
    "        y_true, y_pred, average=\"micro\", zero_division=0\n",
    "    )\n",
    "    metrics[\"micro_recall\"] = recall_score(\n",
    "        y_true, y_pred, average=\"micro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # Macro F1, precision, recall\n",
    "    metrics[\"macro_f1\"] = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    metrics[\"macro_precision\"] = precision_score(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    metrics[\"macro_recall\"] = recall_score(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # Per-class metrics (F1, precision, recall)\n",
    "    unique_classes = sorted(\n",
    "        set([rel[2] for rel in all_relations])\n",
    "    )  # Classes are relation types\n",
    "    class_metrics = {cls: {} for cls in unique_classes}\n",
    "\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        y_true_class = [\n",
    "            1 if rel[2] == cls and rel in expected_relations else 0\n",
    "            for rel in all_relations\n",
    "        ]\n",
    "        y_pred_class = [\n",
    "            1 if rel[2] == cls and rel in predicted_relations else 0\n",
    "            for rel in all_relations\n",
    "        ]\n",
    "\n",
    "        class_metrics[cls][\"f1\"] = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "        class_metrics[cls][\"precision\"] = precision_score(\n",
    "            y_true_class, y_pred_class, zero_division=0\n",
    "        )\n",
    "        class_metrics[cls][\"recall\"] = recall_score(\n",
    "            y_true_class, y_pred_class, zero_division=0\n",
    "        )\n",
    "        class_metrics[cls][\"accuracy\"] = accuracy_score(y_true_class, y_pred_class)\n",
    "\n",
    "    metrics[\"class_metrics\"] = class_metrics\n",
    "\n",
    "    # Overall accuracy\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    #scores_rels.append(metrics)  # For the parallelization\n",
    "    return (\n",
    "        metrics,\n",
    "        {\n",
    "            \"labels\": [\n",
    "                {\n",
    "                    \"head\": head,\n",
    "                    \"tail\": tail,\n",
    "                    \"type\": _type,\n",
    "                    \"correct\": (head, tail, _type) in predicted_relations,\n",
    "                }\n",
    "                for (head, tail, _type) in expected_relations\n",
    "            ],\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                    \"head\": head,\n",
    "                    \"tail\": tail,\n",
    "                    \"type\": _type,\n",
    "                    \"correct\": (head, tail, _type) in expected_relations,\n",
    "                }\n",
    "                for (head, tail, _type) in predicted_relations\n",
    "            ],\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].inputs()['relation_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_type_counter = Counter()\n",
    "\n",
    "for sample in dataset: \n",
    "    relation_types = sample.inputs()['relation_types']\n",
    "    relations_type_counter.update(ast.literal_eval(relation_types))\n",
    "\n",
    "relation_types_count = dict(relations_type_counter)\n",
    "print(relation_types_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(list(relation_types_count.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_set_types = set(list(relation_types_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary keys and values into separate lists for plotting\n",
    "x_values = list(relation_types_count.keys())\n",
    "y_values = list(relation_types_count.values())\n",
    "\n",
    "# Set the style for the plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(x_values, y_values, color=\"skyblue\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Relation Type Frequency', fontsize=16)\n",
    "plt.xlabel('Relation Types', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\", fontsize = 8)  # Rotate x labels for better readability\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels = [None] * 3\n",
    "predictions = [None] * 3\n",
    "label_predictions = [None] * 3\n",
    "incorrect_format = 0\n",
    "\n",
    "for i,sam in tqdm(enumerate(dataset[3:6]), desc=\"Evaluating test data -- Setting baseline\"):\n",
    "    pred = rel_extractor(**sam.inputs())\n",
    "    predictions[i] = pred\n",
    "    scores_rels[i],label_predictions[i] = combined_relation_f1_score(sam, pred)\n",
    "    incorrect_format += len(label_predictions[i][\"predictions\"]) == 0\n",
    "# return scores_rels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def niels_parse_input(input_data):\n",
    "    if isinstance(input_data, str):\n",
    "        print(\"a\")\n",
    "        try:\n",
    "            # Try to parse as JSON\n",
    "            return json.loads(input_data)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # Fallback to ast.literal_eval\n",
    "                return ast.literal_eval(input_data)\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Return None if both parsing attempts fail\n",
    "                return None\n",
    "    elif isinstance(input_data, dict):\n",
    "        print(\"b\")\n",
    "        # Wrap dict in a list\n",
    "        return [input_data]\n",
    "\n",
    "    print(\"c\")\n",
    "    return input_data\n",
    "\n",
    "# for pred in predictions:\n",
    "#     print(pred)\n",
    "#     print(niels_parse_input(pred))\n",
    "#     # print()\n",
    "# for sam in dataset[:3]:\n",
    "#     print(niels_parse_input(sam))\n",
    "\n",
    "# combined_relation_f1_score(sam,\"\"\"[\n",
    "#     {\"entity1\": \"FKT\", \"entity2\": \"multi-class problem\", \"relation\": \"Used for\"},\n",
    "#     {\"entity1\": \"Kernel Fukunaga-Koontz Transform\", \"entity2\": \"face recognition applications\", \"relation\": \"Used for\"},\n",
    "#     {\"entity1\": \"Fukunaga-Koontz Transform\", \"entity2\": \"FKT\",\"relation\": \"Conjunction\"}\n",
    "# ]\"\"\")\n",
    "\n",
    "# label_predictions\n",
    "incorrect_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(program, test_data):\n",
    "    scores_rels = [None] * len(test_data)\n",
    "    label_predictions = []\n",
    "    incorrect_format = [] \n",
    "\n",
    "    for i,sam in tqdm(enumerate(test_data), desc=\"Evaluating test data -- Setting baseline\"):\n",
    "        pred = program(**sam.inputs())\n",
    "        scores_rels[i],lp = combined_relation_f1_score(sam, pred)\n",
    "        if len(lp[\"predictions\"]) == 0:\n",
    "            incorrect_format.append(lp)\n",
    "        else:\n",
    "            label_predictions.append(lp)\n",
    "    return scores_rels,label_predictions,incorrect_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels,label_predictions,incorrect_format = get_score(rel_extractor,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(scores_rels):\n",
    "    # Create a list of dictionaries for the general metrics\n",
    "    general_metrics_data = [None] * len(scores_rels)\n",
    "    for i,item in enumerate(scores_rels):\n",
    "        general_metrics_data[i] = ({\n",
    "            'micro_f1': item['micro_f1'],\n",
    "            'micro_precision': item['micro_precision'],\n",
    "            'micro_recall': item['micro_recall'],\n",
    "            'macro_f1': item['macro_f1'],\n",
    "            'macro_precision': item['macro_precision'],\n",
    "            'macro_recall': item['macro_recall'],\n",
    "            'accuracy': item['accuracy']\n",
    "        })\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    general_metrics_df = pd.DataFrame(general_metrics_data)\n",
    "\n",
    "\n",
    "        # Create a list for class-specific metrics\n",
    "    class_metrics_data = []\n",
    "    for item in scores_rels:\n",
    "        for class_name, metrics in item['class_metrics'].items():\n",
    "            class_metrics_data.append({\n",
    "                'class': class_name,\n",
    "                'f1': metrics['f1'],\n",
    "                'precision': metrics['precision'],\n",
    "                'recall': metrics['recall'],\n",
    "                'accuracy': metrics['accuracy']\n",
    "            })\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    class_metrics_df = pd.DataFrame(class_metrics_data)\n",
    "    return general_metrics_df, class_metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_metrics_df, class_metrics_df = sort_data(scores_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(general_metrics_df, class_metrics_df):\n",
    "    # Set the style for the plots\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Plot for General Metrics\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(30, 10))\n",
    "    fig.suptitle('General Metrics over Experiments', fontsize=16)\n",
    "    \n",
    "    metrics = ['micro_f1', 'micro_precision', 'micro_recall', 'macro_f1', 'macro_precision', 'macro_recall']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        axes[row, col].plot(general_metrics_df.index, general_metrics_df[metric], marker='o')\n",
    "        axes[row, col].set_title(metric)\n",
    "        axes[row, col].set_xlabel('Experiment')\n",
    "        axes[row, col].set_ylabel(metric)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot for Class-Specific Metrics\n",
    "    unique_classes = class_metrics_df['class'].unique()\n",
    "\n",
    "    for metric in ['f1', 'precision', 'recall', 'accuracy']:\n",
    "        plt.figure(figsize=(30, 6))\n",
    "        sns.barplot(data=class_metrics_df, x='class', y=metric)\n",
    "        plt.title(f'Class-Specific {metric.capitalize()}')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_class_occurrences_and_f1(general_metrics_df, class_metrics_df, class_occurrences_dict):\n",
    "    # Set the style for the plots\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Plot for General Metrics\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(30, 10))\n",
    "    fig.suptitle('General Metrics over Experiments', fontsize=16)\n",
    "    \n",
    "    metrics = ['micro_f1', 'micro_precision', 'micro_recall', 'macro_f1', 'macro_precision', 'macro_recall']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        axes[row, col].plot(general_metrics_df.index, general_metrics_df[metric], marker='o')\n",
    "        axes[row, col].set_title(metric)\n",
    "        axes[row, col].set_xlabel('Experiment')\n",
    "        axes[row, col].set_ylabel(metric)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot for Class-Specific Metrics\n",
    "    unique_classes = class_metrics_df['class'].unique()\n",
    "\n",
    "    for metric in ['f1', 'precision', 'recall', 'accuracy']:\n",
    "        plt.figure(figsize=(30, 6))\n",
    "        sns.barplot(data=class_metrics_df, x='class', y=metric)\n",
    "        plt.title(f'Class-Specific {metric.capitalize()}')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Create a new DataFrame that includes the F1 scores and the class occurrences\n",
    "    class_f1_scores = class_metrics_df.groupby('class')['f1'].mean().reindex(class_occurrences_dict.keys())\n",
    "    combined_df = pd.DataFrame({\n",
    "        'class': list(class_occurrences_dict.keys()),\n",
    "        'occurrences': list(class_occurrences_dict.values()),\n",
    "        'f1_score': class_f1_scores\n",
    "    })\n",
    "\n",
    "    # Plot for Class Occurrences with F1 scores\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot occurrences on the left y-axis\n",
    "    ax1.bar(combined_df['class'], combined_df['occurrences'], color=\"lightblue\", label=\"Occurrences\")\n",
    "    ax1.set_xlabel('Class', fontsize=14)\n",
    "    ax1.set_ylabel('Number of Occurrences', fontsize=14, color=\"blue\")\n",
    "    ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # Plot F1 scores on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(combined_df['class'], combined_df['f1_score'], color=\"red\", marker='o', label=\"F1 Score\")\n",
    "    ax2.set_ylabel('F1 Score', fontsize=14, color=\"red\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "\n",
    "    # Add title\n",
    "    plt.title('Number of Times Each Class Occurs with Corresponding F1 Score', fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_metrics_with_class_occurrences_and_f1(general_metrics_df, class_metrics_df, relation_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have general_metrics_df and class_metrics_df already created\n",
    "plot_metrics(general_metrics_df, class_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(score_list):\n",
    "    # Initialize a dictionary to hold the sum of all metrics\n",
    "    avg_metrics = {\n",
    "        'micro_f1': 0,\n",
    "        'micro_precision': 0,\n",
    "        'micro_recall': 0,\n",
    "        'macro_f1': 0,\n",
    "        'macro_precision': 0,\n",
    "        'macro_recall': 0,\n",
    "        'accuracy': 0,\n",
    "        'class_metrics': {}\n",
    "    }\n",
    "    \n",
    "    if not score_list:  # Check if the list is empty\n",
    "        return avg_metrics\n",
    "    \n",
    "    # First, we gather all classes across all score dictionaries\n",
    "    all_classes = set()\n",
    "    for scores in score_list:\n",
    "        all_classes.update(scores['class_metrics'].keys())\n",
    "    \n",
    "    # Initialize the class_metrics to sum the values for each class\n",
    "    for cls in all_classes:\n",
    "        avg_metrics['class_metrics'][cls] = {\n",
    "            'f1': 0,\n",
    "            'precision': 0,\n",
    "            'recall': 0,\n",
    "            'accuracy': 0\n",
    "        }\n",
    "    \n",
    "    # Accumulate metrics across all score dictionaries\n",
    "    for scores in score_list:\n",
    "        avg_metrics['micro_f1'] += scores['micro_f1']\n",
    "        avg_metrics['micro_precision'] += scores['micro_precision']\n",
    "        avg_metrics['micro_recall'] += scores['micro_recall']\n",
    "        avg_metrics['macro_f1'] += scores['macro_f1']\n",
    "        avg_metrics['macro_precision'] += scores['macro_precision']\n",
    "        avg_metrics['macro_recall'] += scores['macro_recall']\n",
    "        avg_metrics['accuracy'] += scores['accuracy']\n",
    "        \n",
    "        # Accumulate class metrics\n",
    "        for cls, metrics in scores['class_metrics'].items():\n",
    "            avg_metrics['class_metrics'][cls]['f1'] += metrics['f1']\n",
    "            avg_metrics['class_metrics'][cls]['precision'] += metrics['precision']\n",
    "            avg_metrics['class_metrics'][cls]['recall'] += metrics['recall']\n",
    "            avg_metrics['class_metrics'][cls]['accuracy'] += metrics['accuracy']\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_scores = len(score_list)\n",
    "    \n",
    "    if num_scores > 0:  # Prevent division by zero\n",
    "        avg_metrics['micro_f1'] /= num_scores\n",
    "        avg_metrics['micro_precision'] /= num_scores\n",
    "        avg_metrics['micro_recall'] /= num_scores\n",
    "        avg_metrics['macro_f1'] /= num_scores\n",
    "        avg_metrics['macro_precision'] /= num_scores\n",
    "        avg_metrics['macro_recall'] /= num_scores\n",
    "        avg_metrics['accuracy'] /= num_scores\n",
    "    \n",
    "        # Calculate per-class averages\n",
    "        for cls in avg_metrics['class_metrics']:\n",
    "            avg_metrics['class_metrics'][cls]['f1'] /= num_scores\n",
    "            avg_metrics['class_metrics'][cls]['precision'] /= num_scores\n",
    "            avg_metrics['class_metrics'][cls]['recall'] /= num_scores\n",
    "            avg_metrics['class_metrics'][cls]['accuracy'] /= num_scores\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_metrics(scores_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the data (scores_rels, general_metrics_df, class_metrics_df) with custom name\n",
    "def save_data(scores_rels, general_metrics_df, class_metrics_df, label_predictions, incorrect_format, folder_path, name):\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Save scores_rels as a pickle file\n",
    "    scores_rels_filepath = os.path.join(folder_path, f\"dspy_{name}_scores_rels.pkl\")\n",
    "    with open(scores_rels_filepath, \"wb\") as handle:\n",
    "        pickle.dump(scores_rels, handle)\n",
    "    \n",
    "    # Save general_metrics_df as a pickle file\n",
    "    general_metrics_filepath = os.path.join(folder_path, f\"dspy_{name}_general_metrics_df.pkl\")\n",
    "    general_metrics_df.to_pickle(general_metrics_filepath)\n",
    "    \n",
    "    # Save class_metrics_df as a pickle file\n",
    "    class_metrics_filepath = os.path.join(folder_path, f\"dspy_{name}_general_class_metrics_df.pkl\")\n",
    "    class_metrics_df.to_pickle(class_metrics_filepath)\n",
    "\n",
    "    # save label_predictions as a pickle file\n",
    "    label_predictions_filepath= os.path.join(folder_path, f\"dspy_{name}_label_predictions.pkl\")\n",
    "    with open(label_predictions_filepath, \"wb\") as f:\n",
    "        pickle.dump(label_predictions, f)\n",
    "    \n",
    "    # save incorrect_format as a pickle file\n",
    "    incorrect_format_filepath = os.path.join(folder_path, f\"dspy_{name}_incorrect_format.pkl\")\n",
    "    with open(incorrect_format_filepath,\"wb\") as f:\n",
    "        pickle.dump(incorrect_format,f)\n",
    "    \n",
    "    # Return the paths of the saved files\n",
    "    return scores_rels_filepath, general_metrics_filepath, class_metrics_filepath, label_predictions_filepath, incorrect_format_filepath\n",
    "\n",
    "# Example usage:\n",
    "# scores_rels, general_metrics_df, class_metrics_df are assumed to be your datasets\n",
    "save_data(scores_rels, general_metrics_df, class_metrics_df, label_predictions,incorrect_format,\"/work3/s174159/LLM_Thesis/plots/data\", \"dspy_0_shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relation_f1_score(expected, predicted, trace=None):\n",
    "    def parse_input(input_data):\n",
    "        if isinstance(input_data, str):\n",
    "            try:\n",
    "                # Try to parse as JSON\n",
    "                return json.loads(input_data)\n",
    "            except json.JSONDecodeError:\n",
    "                try:\n",
    "                    # Fallback to ast.literal_eval\n",
    "                    return ast.literal_eval(input_data)\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # Return None if both parsing attempts fail\n",
    "                    return None\n",
    "        elif isinstance(input_data, dict):\n",
    "            # Wrap dict in a list\n",
    "            return [input_data]\n",
    "        return input_data\n",
    "\n",
    "    # Parse expected and predicted inputs\n",
    "    expected = parse_input(expected)\n",
    "    predicted = parse_input(predicted)\n",
    "\n",
    "    # print(\"Predicted\", predicted)\n",
    "\n",
    "\n",
    "\n",
    "    expected_relations = set()\n",
    "\n",
    "    \n",
    "    entities = expected.entities\n",
    "    relations = expected.relations\n",
    "    \n",
    "    flat_entities = [entity for sublist in entities for entity in sublist]\n",
    "    for rel in relations:\n",
    "        entity1_name = flat_entities[rel['h']]['name']\n",
    "        entity2_name = flat_entities[rel['t']]['name']\n",
    "        relation_type = rel['r']\n",
    "        expected_relations.add((entity1_name, entity2_name, relation_type))\n",
    "    \n",
    "    # print(\"expected relations\", expected_relations)\n",
    "    \n",
    "    # Extract predicted relations\n",
    "    predicted_relations = {\n",
    "        (rel['entity1'], rel['entity2'], rel['relation'])\n",
    "        for rel in predicted\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create binary labels for true/false positives\n",
    "    y_true = [1 if rel in expected_relations else 0 for rel in predicted_relations]\n",
    "    y_pred = [1] * len(predicted_relations)  # All predicted relations are labeled as 1\n",
    "\n",
    "    # Handle case where there are no true relations in expected\n",
    "    if not y_true:\n",
    "        return 0\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    if not f1:\n",
    "        return 0\n",
    "    return f1 #,expected_relations, predicted_relations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4oMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp3 = BootstrapFewShotWithRandomSearch(metric=relation_f1_score, max_bootstrapped_demos=2, max_labeled_demos=1, num_candidate_programs=5, num_threads=8, max_rounds = 1)\n",
    "optimized_program_3 = tp3.compile(rel_extractor, trainset=dataset, valset=dev_data, restrict=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program_3.save(\"/work3/s174159/LLM_Thesis/prompt/opt_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program_3 = rel_extractor\n",
    "optimized_program_3.load(\"/work3/s174159/LLM_Thesis/prompt/opt_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels,label_predictions,incorrect_format = get_score(optimized_program_3,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have general_metrics_df and class_metrics_df already created\n",
    "general_metrics_df, class_metrics_df = sort_data(scores_rels)\n",
    "plot_metrics(general_metrics_df, class_metrics_df)\n",
    "average_metrics(scores_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(scores_rels, general_metrics_df, class_metrics_df,label_predictions,incorrect_format, \"/work3/s174159/LLM_Thesis/plots/data\", \"3_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp6 = BootstrapFewShotWithRandomSearch(metric=relation_f1_score, max_bootstrapped_demos=6, max_labeled_demos=3, num_candidate_programs=5, num_threads=8, max_rounds = 1)\n",
    "optimized_program_6 = tp6.compile(rel_extractor, trainset=dataset, valset=dev_data, restrict=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program_6.save(\"/work3/s174159/LLM_Thesis/prompt/opt_6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels,label_predictions,incorrect_format = get_score(optimized_program_6,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have general_metrics_df and class_metrics_df already created\n",
    "general_metrics_df, class_metrics_df = sort_data(scores_rels)\n",
    "plot_metrics(general_metrics_df, class_metrics_df)\n",
    "average_metrics(scores_rels)\n",
    "\n",
    "save_data(scores_rels, general_metrics_df, class_metrics_df,label_predictions,incorrect_format, \"/work3/s174159/LLM_Thesis/plots/data\", \"6_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp10 = BootstrapFewShotWithRandomSearch(metric=relation_f1_score, max_bootstrapped_demos=10, max_labeled_demos=5, num_candidate_programs=5, num_threads=8, max_rounds = 1)\n",
    "optimized_program_10 = tp10.compile(rel_extractor, trainset=dataset, valset=dev_data, restrict=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program_6.save(\"/work3/s174159/LLM_Thesis/prompt/opt_6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels,label_predictions,incorrect_format  = get_score(optimized_program_10,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have general_metrics_df and class_metrics_df already created\n",
    "general_metrics_df, class_metrics_df = sort_data(scores_rels)\n",
    "plot_metrics(general_metrics_df, class_metrics_df)\n",
    "average_metrics(scores_rels)\n",
    "\n",
    "save_data(scores_rels, general_metrics_df, class_metrics_df,label_predictions,incorrect_format, \"/work3/s174159/LLM_Thesis/plots/data\", \"10_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp3 = BootstrapFewShotWithRandomSearch(metric=relation_f1_score, max_bootstrapped_demos=2, max_labeled_demos=1, num_candidate_programs=5, num_threads=8, max_rounds = 1)\n",
    "goptimized_program_3 = gtp3.compile(rel_extractor, trainset=dataset, valset=dev_data, restrict=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goptimized_program_3.save(\"/work3/s174159/LLM_Thesis/prompt/gopt_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goptimized_program_3 = rel_extractor\n",
    "goptimized_program_3.load(\"/work3/s174159/LLM_Thesis/prompt/gopt_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels,label_predictions,incorrect_format = get_score(goptimized_program_3,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have general_metrics_df and class_metrics_df already created\n",
    "general_metrics_df, class_metrics_df = sort_data(scores_rels)\n",
    "plot_metrics(general_metrics_df, class_metrics_df)\n",
    "average_metrics(scores_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(scores_rels, general_metrics_df, class_metrics_df,label_predictions,incorrect_format, \"/work3/s174159/LLM_Thesis/plots/data\", \"g3_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp6 = BootstrapFewShotWithRandomSearch(metric=relation_f1_score, max_bootstrapped_demos=6, max_labeled_demos=3, num_candidate_programs=5, num_threads=8, max_rounds = 1)\n",
    "goptimized_program_6 = gtp6.compile(rel_extractor, trainset=dataset, valset=dev_data, restrict=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goptimized_program_6.save(\"/work3/s174159/LLM_Thesis/prompt/gopt_6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels,label_predictions,incorrect_format = get_score(goptimized_program_6,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have general_metrics_df and class_metrics_df already created\n",
    "general_metrics_df, class_metrics_df = sort_data(scores_rels)\n",
    "plot_metrics(general_metrics_df, class_metrics_df)\n",
    "average_metrics(scores_rels)\n",
    "\n",
    "save_data(scores_rels, general_metrics_df, class_metrics_df,label_predictions,incorrect_format, \"/work3/s174159/LLM_Thesis/plots/data\", \"g6_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp10 = BootstrapFewShotWithRandomSearch(metric=relation_f1_score, max_bootstrapped_demos=10, max_labeled_demos=5, num_candidate_programs=5, num_threads=8, max_rounds = 1)\n",
    "goptimized_program_10 = gtp10.compile(rel_extractor, trainset=dataset, valset=dev_data, restrict=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program_6.save(\"/work3/s174159/LLM_Thesis/prompt/gopt_6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rels,label_predictions,incorrect_format  = get_score(goptimized_program_10,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have general_metrics_df and class_metrics_df already created\n",
    "general_metrics_df, class_metrics_df = sort_data(scores_rels)\n",
    "plot_metrics(general_metrics_df, class_metrics_df)\n",
    "average_metrics(scores_rels)\n",
    "\n",
    "save_data(scores_rels, general_metrics_df, class_metrics_df,label_predictions,incorrect_format, \"/work3/s174159/LLM_Thesis/plots/data\", \"g10_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.HFClientVLLM(model=\"meta-llama/Llama-2-70b-chat-hf\", port=8000, url=\"http://localhost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp3 = BootstrapFewShotWithRandomSearch(metric=relation_f1_score, max_bootstrapped_demos=2, max_labeled_demos=1, num_candidate_programs=5, num_threads=8, max_rounds = 1)\n",
    "goptimized_program_3 = gtp3.compile(rel_extractor, trainset=dataset, valset=dev_data, restrict=range(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
